{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Myloss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFFMnJZS6GZNDImF+PDETK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alisonwqq/Zero-DCE_extension/blob/main/Myloss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krzSpKsPlx9R"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torchvision.models.vgg import vgg16\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class L_color(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(L_color, self).__init__()\n",
        "\n",
        "    def forward(self, x ):\n",
        "\n",
        "        b,c,h,w = x.shape\n",
        "\n",
        "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
        "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
        "        Drg = torch.pow(mr-mg,2)\n",
        "        Drb = torch.pow(mr-mb,2)\n",
        "        Dgb = torch.pow(mb-mg,2)\n",
        "        k = torch.pow(torch.pow(Drg,2) + torch.pow(Drb,2) + torch.pow(Dgb,2),0.5)\n",
        "\n",
        "\n",
        "        return k\n",
        "\n",
        "\t\t\t\n",
        "class L_spa(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(L_spa, self).__init__()\n",
        "        # print(1)kernel = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n",
        "        kernel_left = torch.FloatTensor( [[0,0,0],[-1,1,0],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
        "        kernel_right = torch.FloatTensor( [[0,0,0],[0,1,-1],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
        "        kernel_up = torch.FloatTensor( [[0,-1,0],[0,1, 0 ],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
        "        kernel_down = torch.FloatTensor( [[0,0,0],[0,1, 0],[0,-1,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
        "        self.weight_left = nn.Parameter(data=kernel_left, requires_grad=False)\n",
        "        self.weight_right = nn.Parameter(data=kernel_right, requires_grad=False)\n",
        "        self.weight_up = nn.Parameter(data=kernel_up, requires_grad=False)\n",
        "        self.weight_down = nn.Parameter(data=kernel_down, requires_grad=False)\n",
        "        self.pool = nn.AvgPool2d(4)\n",
        "    def forward(self, org , enhance ):\n",
        "        b,c,h,w = org.shape\n",
        "\n",
        "        org_mean = torch.mean(org,1,keepdim=True)\n",
        "        enhance_mean = torch.mean(enhance,1,keepdim=True)\n",
        "\n",
        "        org_pool =  self.pool(org_mean)\t\t\t\n",
        "        enhance_pool = self.pool(enhance_mean)\t\n",
        "\n",
        "        weight_diff =torch.max(torch.FloatTensor([1]).cuda() + 10000*torch.min(org_pool - torch.FloatTensor([0.3]).cuda(),torch.FloatTensor([0]).cuda()),torch.FloatTensor([0.5]).cuda())\n",
        "        E_1 = torch.mul(torch.sign(enhance_pool - torch.FloatTensor([0.5]).cuda()) ,enhance_pool-org_pool)\n",
        "\n",
        "\n",
        "        D_org_letf = F.conv2d(org_pool , self.weight_left, padding=1)\n",
        "        D_org_right = F.conv2d(org_pool , self.weight_right, padding=1)\n",
        "        D_org_up = F.conv2d(org_pool , self.weight_up, padding=1)\n",
        "        D_org_down = F.conv2d(org_pool , self.weight_down, padding=1)\n",
        "\n",
        "        D_enhance_letf = F.conv2d(enhance_pool , self.weight_left, padding=1)\n",
        "        D_enhance_right = F.conv2d(enhance_pool , self.weight_right, padding=1)\n",
        "        D_enhance_up = F.conv2d(enhance_pool , self.weight_up, padding=1)\n",
        "        D_enhance_down = F.conv2d(enhance_pool , self.weight_down, padding=1)\n",
        "\n",
        "        D_left = torch.pow(D_org_letf - D_enhance_letf,2)\n",
        "        D_right = torch.pow(D_org_right - D_enhance_right,2)\n",
        "        D_up = torch.pow(D_org_up - D_enhance_up,2)\n",
        "        D_down = torch.pow(D_org_down - D_enhance_down,2)\n",
        "        E = (D_left + D_right + D_up +D_down)\n",
        "        # E = 25*(D_left + D_right + D_up +D_down)\n",
        "\n",
        "        return E\n",
        "class L_exp(nn.Module):\n",
        "\n",
        "    def __init__(self,patch_size):\n",
        "        super(L_exp, self).__init__()\n",
        "        # print(1)\n",
        "        self.pool = nn.AvgPool2d(patch_size)\n",
        "        # self.mean_val = mean_val\n",
        "    def forward(self, x, mean_val ):\n",
        "\n",
        "        b,c,h,w = x.shape\n",
        "        x = torch.mean(x,1,keepdim=True)\n",
        "        mean = self.pool(x)\n",
        "\n",
        "        d = torch.mean(torch.pow(mean- torch.FloatTensor([mean_val] ).cuda(),2))\n",
        "        return d\n",
        "        \n",
        "class L_TV(nn.Module):\n",
        "    def __init__(self,TVLoss_weight=1):\n",
        "        super(L_TV,self).__init__()\n",
        "        self.TVLoss_weight = TVLoss_weight\n",
        "\n",
        "    def forward(self,x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h =  (x.size()[2]-1) * x.size()[3]\n",
        "        count_w = x.size()[2] * (x.size()[3] - 1)\n",
        "        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n",
        "        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n",
        "        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n",
        "class Sa_Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sa_Loss, self).__init__()\n",
        "        # print(1)\n",
        "    def forward(self, x ):\n",
        "        # self.grad = np.ones(x.shape,dtype=np.float32)\n",
        "        b,c,h,w = x.shape\n",
        "        # x_de = x.cpu().detach().numpy()\n",
        "        r,g,b = torch.split(x , 1, dim=1)\n",
        "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
        "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
        "        Dr = r-mr\n",
        "        Dg = g-mg\n",
        "        Db = b-mb\n",
        "        k =torch.pow( torch.pow(Dr,2) + torch.pow(Db,2) + torch.pow(Dg,2),0.5)\n",
        "        # print(k)\n",
        "        \n",
        "\n",
        "        k = torch.mean(k)\n",
        "        return k\n",
        "\n",
        "class perception_loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(perception_loss, self).__init__()\n",
        "        # vgg = vgg16(pretrained=True).cuda()\n",
        "        features = vgg16(pretrained=True).features\n",
        "        self.to_relu_1_2 = nn.Sequential() \n",
        "        self.to_relu_2_2 = nn.Sequential() \n",
        "        self.to_relu_3_3 = nn.Sequential()\n",
        "        self.to_relu_4_3 = nn.Sequential()\n",
        "\n",
        "        for x in range(4):\n",
        "            self.to_relu_1_2.add_module(str(x), features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.to_relu_2_2.add_module(str(x), features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.to_relu_3_3.add_module(str(x), features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.to_relu_4_3.add_module(str(x), features[x])\n",
        "        \n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.to_relu_1_2(x)\n",
        "        h_relu_1_2 = h\n",
        "        h = self.to_relu_2_2(h)\n",
        "        h_relu_2_2 = h\n",
        "        h = self.to_relu_3_3(h)\n",
        "        h_relu_3_3 = h\n",
        "        h = self.to_relu_4_3(h)\n",
        "        h_relu_4_3 = h\n",
        "        # out = (h_relu_1_2, h_relu_2_2, h_relu_3_3, h_relu_4_3)\n",
        "        return h_relu_4_3"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}